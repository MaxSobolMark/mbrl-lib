# @package _group_
env: "drawer_lifelong_learning_fsrl"
num_distinct_envs: 10
envs:
  -
    env_name: "metaworld___drawer-open-v2-max"
    wrappers: []
    task_name: "open_task_1"
    reward_fn: "metaworld_drawer_open"
    env_kwargs:
      task_number: 1
      num_tasks: 5
    task_index: 1
  -
    env_name: "metaworld___drawer-close-v2-max"
    wrappers: []
    task_name: "close_task_1"
    reward_fn: "metaworld_drawer_close"
    env_kwargs:
      task_number: 1
      num_tasks: 5
    task_index: 1
  -
    env_name: "metaworld___drawer-open-v2-max"
    wrappers: []
    task_name: "open_task_3"
    reward_fn: "metaworld_drawer_open"
    env_kwargs:
      task_number: 3
      num_tasks: 5
    task_index: 3
  -
    env_name: "metaworld___drawer-close-v2-max"
    wrappers: []
    task_name: "close_task_4"
    reward_fn: "metaworld_drawer_close"
    env_kwargs:
      task_number: 4
      num_tasks: 5
    task_index: 4
  -
    env_name: "metaworld___drawer-open-v2-max"
    wrappers: []
    task_name: "open_task_4"
    reward_fn: "metaworld_drawer_open"
    env_kwargs:
      task_number: 4
      num_tasks: 5
    task_index: 4
  -
    env_name: "metaworld___drawer-close-v2-max"
    wrappers: []
    task_name: "close_task_3"
    reward_fn: "metaworld_drawer_close"
    env_kwargs:
      task_number: 3
      num_tasks: 5
    task_index: 3
  -
    env_name: "metaworld___drawer-open-v2-max"
    wrappers: []
    task_name: "open_task_2"
    reward_fn: "metaworld_drawer_open"
    env_kwargs:
      task_number: 2
      num_tasks: 5
    task_index: 2
  -
    env_name: "metaworld___drawer-close-v2-max"
    wrappers: []
    task_name: "close_task_2"
    reward_fn: "metaworld_drawer_close"
    env_kwargs:
      task_number: 2
      num_tasks: 5
    task_index: 2
  -
    env_name: "metaworld___drawer-open-v2-max"
    wrappers: []
    task_name: "open_task_5"
    reward_fn: "metaworld_drawer_open"
    env_kwargs:
      task_number: 5
      num_tasks: 5
    task_index: 5
  -
    env_name: "metaworld___drawer-close-v2-max"
    wrappers: []
    task_name: "close_task_5"
    reward_fn: "metaworld_drawer_close"
    env_kwargs:
      task_number: 5
      num_tasks: 5
    task_index: 5
term_fn: "no_termination"
policy_to_use: "COMBINED"
learned_rewards: false
num_steps: 100000
trial_length: 500
epoch_length: 500

num_elites: 5
model_lr: 0.001  # 4e-4
model_wd: 0  # 1.7e-4
model_batch_size: 32
validation_ratio: 0.  # 0.05
no_delta_list: []  # [7, 8, 9]  # [ 0 ]
freq_train_model: 125  # 250
patience: 5  # 25
num_epochs_train_model: 5  # 25

planning_horizon: 30
cem_num_iters: 5
cem_elite_ratio: 0.1
cem_population_size: 500  # 350
cem_alpha: 0.1

# Dynamics model config
observe_task_id: false
forward_postprocess_fn: None

# MBPO Config
effective_model_rollouts_per_step: 400
rollout_schedule: [20, 150, 1, 1]
num_sac_updates_per_step: 20
sac_updates_every_steps: 1
num_epochs_to_retain_sac_buffer: 1

sac_batch_size: 500
sac_alpha_lr: 0.0003
sac_actor_lr: 0.0003
sac_actor_update_frequency: 4
sac_critic_lr: 0.00003
sac_critic_target_update_frequency: 4
sac_target_entropy: -3
sac_hidden_depth: 2
sac_hidden_dim: 256

# Switching mechanism config
# in ["last_n_episodes", "current_observation"]
returns_expectation_calculation_method: "last_n_episodes"
returns_expectation_n_episodes: 3

planning_mopo_penalty_coeff: 0
